---
title: 从 0 开始学大数据
categories:
  - 笔记
  - 大数据
tags:
  - 大数据
---

# 从 0 开始学大数据

## 01 丨预习 01 丨大数据技术发展史：大数据的前世今生

大数据技术，起源于 Google 在 2004 年前后发表的三篇论文，分别是分布式文件系统 GFS、大数据分布式计算框架
MapReduce 和 NoSQL 数据库系统 BigTable。

Doug Cutting 根据 Google 论文开发了 Hadoop。

大数据处理的主要应用场景包括数据分析、数据挖掘与机器学习。

数据分析主要使用 Hive、Spark SQL 等 SQL 引擎完成；

数据挖掘与机器学习则有专门的机器学习框架 TensorFlow、Mahout 以及 MLlib 等，内置了主要的机器学习和数据挖掘算法。

大数据要存入分布式文件系统（HDFS），要有序调度 MapReduce 和 Spark 作业执行，并能把执行结果写入到各个应用系统的数据库中，还需要有一个大数据平台整合所有这些大数据组件和企业应用系统。

![](https://raw.githubusercontent.com/dunwu/images/dev/snap/20230223134708.png)

## 预习 02 | 大数据应用发展史：从搜索引擎到人工智能

大数据的应用领域：

- **搜索引擎**：GFS 和 MapReduce 开启了超大规模的分布式存储和分布式计算应用。
- **数据仓库**：Hive 实现了用更低廉的价格获得比以往多得多的数据存储与计算能力。
- **数据挖掘**：基于海量数据进行关联分析。应用有：关联推荐、用户画像、关系图谱
- **机器学习**：有了大数据，可以把全部的历史数据都收集起来，统计其规律，进而预测正在发生的事情。

## 03 丨预习 03 丨大数据应用领域：数据驱动一切

大数据的行业应用：

- 医疗健康领域
  - 医学影像智能识别
  - 病历大数据智能诊疗
- 教育领域
  - AI 外语老师
  - 智能解题
- 社交媒体领域：舆情监控与分析
- 金融领域：大数据风控
- 新零售领域：全链路管理
- 交通领域
  - 实时采集监控数据
  - 判断道路拥堵状态
  - 无人驾驶技术

## 04 | 移动计算比移动数据更划算

传统计算处理模型，都是“输入 -> 计算 -> 输出”模型。不适用于互联网时代的 PB 级的计算要求。

大数据计算处理通常针对的是网站的存量数据。网站大数据系统要做的就是将这些统计规律和关联关系
计算出来，并由此进一步改善网站的用户体验和运营决策。

移动计算：将程序分发到数据所在的地方进行计算，也就是所谓的移动计算比移动数据更划算。

移动计算步骤

1. 将待处理的大规模数据存储在服务器集群的所有服务器上，主要使用 HDFS 分布式文件
   存储系统，将文件分成很多块（Block），以块为单位存储在集群的服务器上。
2. 大数据引擎根据集群里不同服务器的计算能力，在每台服务器上启动若干分布式任务执
   行进程，这些进程会等待给它们分配执行任务。
3. 使用大数据计算框架支持的编程模型进行编程，比如 Hadoop 的 MapReduce 编程模
   型，或者 Spark 的 RDD 编程模型。应用程序编写好以后，将其打包，MapReduce 和
   Spark 都是在 JVM 环境中运行，所以打包出来的是一个 Java 的 JAR 包。
4. 用 Hadoop 或者 Spark 的启动命令执行这个应用程序的 JAR 包，首先执行引擎会解析
   程序要处理的数据输入路径，根据输入数据量的大小，将数据分成若干片（Split），每一
   个数据片都分配给一个任务执行进程去处理。
5. 任务执行进程收到分配的任务后，检查自己是否有任务对应的程序包，如果没有就去下
   载程序包，下载以后通过反射的方式加载程序。走到这里，最重要的一步，也就是移动计算
   就完成了。
6. 加载程序后，任务执行进程根据分配的数据片的文件地址和数据在文件内的偏移量读取
   数据，并把数据输入给应用程序相应的方法

## 05 | 从 RAID 看垂直伸缩到水平伸缩的演化

大规模数据存储核心问题

- 数据存储容量的问题。既然大数据要解决的是数以 PB 计的数据计算问题，而一般的服务器磁盘容量通常 1 ～ 2TB，那么如何存储这么大规模的数据呢？
- 数据读写速度的问题。一般磁盘的连续读写速度为几十 MB，以这样的速度，几十 PB 的数据恐怕要读写到天荒地老。
- 数据可靠性的问题。磁盘大约是计算机设备中最易损坏的硬件了，通常情况一块磁盘使用寿命大概是一年，如果磁盘损坏了，数据怎么办？

解决方式是 RAID 技术：

- 数据存储容量的问题。RAID 使用了 N 块磁盘构成一个存储阵列，如果使用 RAID 5，数
  据就可以存储在 N-1 块磁盘上，这样将存储空间扩大了 N-1 倍。
- 数据读写速度的问题。RAID 根据可以使用的磁盘数量，将待写入的数据分成多片，并发
  同时向多块磁盘进行写入，显然写入的速度可以得到明显提高；同理，读取速度也可以得到
  明显提高。不过，需要注意的是，由于传统机械磁盘的访问延迟主要来自于寻址时间，数据
  真正进行读写的时间可能只占据整个数据访问时间的一小部分，所以数据分片后对 N 块磁
  盘进行并发读写操作并不能将访问速度提高 N 倍。
- 数据可靠性的问题。使用 RAID 10、RAID 5 或者 RAID 6 方案的时候，由于数据有冗余
  存储，或者存储校验信息，所以当某块磁盘损坏的时候，可以通过其他磁盘上的数据和校验
  数据将丢失磁盘上的数据还原。

![](https://raw.githubusercontent.com/dunwu/images/dev/snap/20230224131454.png)

实现更强的计算能力和更大规模的数据存储有两种思路

- 垂直伸缩（scaling up），即硬件升级
- 水平伸缩（scaling out），即分布式系统

注：RAID 技术就是采用了垂直伸缩的方式。

## 06 | 新技术层出不穷，HDFS 依然是存储的王者

![img](https://raw.githubusercontent.com/dunwu/images/dev/cs/bigdata/hdfs/hdfs-architecture.png)

HDFS 有两个关键组件：DataNode 和 NameNode：

- DataNode 负责文件数据的存储和读写操作，HDFS 将文件数据分割成若干数据块（Block），每个 DataNode 存储一部分数据块，这样文件就分布存储在整个 HDFS 服务器集群中。应用程序客户端（Client）可以并行对这些数据块进行访问，从而使得 HDFS 可以在服务器集群规模上实现数据并行访问，极大地提高了访问速度。
- NameNode 负责整个分布式文件系统的元数据（MetaData）管理，也就是文件路径名、数据块的 ID 以及存储位置等信息，相当于操作系统中文件分配表（FAT）的角色。

为保证高可用，HDFS 会，会将一个数据块复制为多份（默认为 3 份），并将多份相同的数据块存储在不同的服务器上，甚至不同的机架上。

![img](https://raw.githubusercontent.com/dunwu/images/dev/snap/20200224203958.png)

1. 数据存储故障容错
   磁盘介质在存储过程中受环境或者老化影响，其存储的数据可能会出现错乱。HDFS 的应对措施是，对于存储在 DataNode 上的数据块，计算并存储校验和（CheckSum）。在读取 数据的时候，重新计算读取出来的数据的校验和，如果校验不正确就抛出异常，应用程序捕 获异常后就到其他 DataNode 上读取备份数据。
2. 磁盘故障容错
   如果 DataNode 监测到本机的某块磁盘损坏，就将该块磁盘上存储的所有 BlockID 报告给 NameNode，NameNode 检查这些数据块还在哪些 DataNode 上有备份，通知相应的 DataNode 服务器将对应的数据块复制到其他服务器上，以保证数据块的备份数满足要求。
3. DataNode 故障容错
   DataNode 会通过心跳和 NameNode 保持通信，如果 DataNode 超时未发送心跳， NameNode 就会认为这个 DataNode 已经宕机失效，立即查找这个 DataNode 上存储的数据块有哪些，以及这些数据块还存储在哪些服务器上，随后通知这些服务器再复制一份数据块到其他服务器上，保证 HDFS 存储的数据块备份数符合用户设置的数目，即使再出现服务器宕机，也不会丢失数据。
4. NameNode 故障容错
   NameNode 是整个 HDFS 的核心，记录着 HDFS 文件分配表信息，所有的文件路径和数据块存储信息都保存在 NameNode，如果 NameNode 故障，整个 HDFS 系统集群都无法使用；如果 NameNode 上记录的数据丢失，整个集群所有 DataNode 存储的数据也就没用了。

NameNode 采用主从热备的方式提供高可用服务。

集群部署两台 NameNode 服务器，一台作为主服务器提供服务，一台作为从服务器进行热备，两台服务器通过 ZooKeeper 选举，主要是通过争夺 znode 锁资源，决定谁是主服务器。而 DataNode 则会向两个 NameNode 同时发送心跳数据，但是只有主 NameNode 才能向 DataNode 返回控制信息。

## 07 | 为什么说 MapReduce 既是编程模型又是计算框架？

MapReduce 编程模型只包含 Map 和 Reduce 两个过程，map 的主要输入是一对 `<Key, Value>` 值，经过 map 计算后输出一对 `<Key, Value>` 值；然后将相同 Key 合并，形成 `<Key, Value 集合>`；再将这个 `<Key, Value 集合>` 输入 reduce，经过计算输出零个或多个 `<Key, Value>` 对。

## 08 | MapReduce 如何让数据完成一次旅行？

### MapReduce 作业启动和运行机制

大数据应用进程。这类进程是启动 MapReduce 程序的主入口，主要是指定 Map 和 Reduce 类、输入输出文件路径等，并提交作业给 Hadoop 集群，也就是下面提到的 JobTracker 进程。这是由用户启动的 MapReduce 程序进程，比如我们上期提到的 WordCount 程序。

JobTracker 进程。这类进程根据要处理的输入数据量，命令下面提到的 TaskTracker 进程启动相应数量的 Map 和 Reduce 进程任务，并管理整个作业生命周期的任务调度和监控。这是 Hadoop 集群的常驻进程，需要注意的是，JobTracker 进程在整个 Hadoop 集群全局唯一。

TaskTracker 进程。这个进程负责启动和管理 Map 进程以及 Reduce 进程。因为需要每个数据块都有对应的 map 函数，TaskTracker 进程通常和 HDFS 的 DataNode 进程启动在同一个服务器。也就是说，Hadoop 集群中绝大多数服务器同时运行 DataNode 进程和 TaskTracker 进程。

### MapReduce 数据合并与连接机制

在 map 输出与 reduce 输入之间，MapReduce 计算框架处理数据合并与连接操作，这个操作有个专门的词汇叫 shuffle。分布式计算需要将不同服务器上的相关数据合并到一起进行下一步计算，这就是 shuffle。

## 09 | 为什么我们管 Yarn 叫作资源调度框架？

服务器集群资源调度管理和 MapReduce 执行过程耦合在一起，如果想在当前集群中运行其他计算任务，比如 Spark 或者 Storm，就无法统一使用集群中的资源了。

![](https://raw.githubusercontent.com/dunwu/images/dev/snap/20230227195344.png)

Yarn 包括两个部分：

ResourceManager 进程负责整个集群的资源调度管理，通常部署在独立的服务器上；

NodeManager 进程负责具体服务器上的资源和任务管理，在集群的每一台计算服务器上都会启动，基本上跟 HDFS 的 DataNode 进程一起出现。

Yarn 的工作流程

1. 我们向 Yarn 提交应用程序，包括 MapReduce ApplicationMaster、我们的 MapReduce 程序，以及 MapReduce Application 启动命令。
2. ResourceManager 进程和 NodeManager 进程通信，根据集群资源，为用户程序分配第一个容器，并将 MapReduce ApplicationMaster 分发到这个容器上面，并在容器里面启动 MapReduce ApplicationMaster。
3. MapReduce ApplicationMaster 启动后立即向 ResourceManager 进程注册，并为自己的应用程序申请容器资源。
4. MapReduce ApplicationMaster 申请到需要的容器后，立即和相应的 NodeManager 进程通信，将用户 MapReduce 程序分发到 NodeManager 进程所在服务器，并在容器中运行，运行的就是 Map 或者 Reduce 任务。
5. Map 或者 Reduce 任务在运行期和 MapReduce ApplicationMaster 通信，汇报自己的运行状态，如果运行结束，MapReduce ApplicationMaster 向 ResourceManager 进程注销并释放所有的容器资源。

## 10 | 模块答疑：我们能从 Hadoop 学到什么？

Hadoop 几个主要产品的架构设计，就会发现它们都有相似性，都是一主多从的架构方案。

- HDFS，一个 NameNode，多个 DataNode；
- MapReduce 1，一个 JobTracker，多个 TaskTracker；
- Yarn，一个 ResourceManager，多个 NodeManager。

事实上，很多大数据产品都是这样的架构方案：

Storm，一个 Nimbus，多个 Supervisor；

Spark，一个 Master，多个 Slave。

大数据因为要对数据和计算任务进行统一管理，所以和互联网在线应用不同，需要一个全局管理者。一言以蔽之：**集中管理，分布存储与计算**

## 11 | Hive 是如何让 MapReduce 实现 SQL 操作的？

Hive 能够直接处理我们输入的 SQL 语句（Hive 的 SQL 语法和数据库标准 SQL 略有不同），调用 MapReduce 计算框架完成数据分析操作。

我们通过 Hive 的 Client（Hive 的命令行工具，JDBC 等）向 Hive 提交 SQL 命令。如果是创建数据表的 DDL（数据定义语言），Hive 就会通过执行引擎 Driver 将数据表的信息记录在 Metastore 元数据组件中，这个组件通常用一个关系数据库实现，记录表名、字段名、字段类型、关联 HDFS 文件路径等这些数据库的 Meta 信息（元信息）。

如果我们提交的是查询分析数据的 DQL（数据查询语句），Driver 就会将该语句提交给自己的编译器 Compiler 进行语法分析、语法解析、语法优化等一系列操作，最后生成一个 MapReduce 执行计划。然后根据执行计划生成一个 MapReduce 的作业，提交给 Hadoop MapReduce 计算框架处理。

## 12 | 我们并没有觉得 MapReduce 速度慢，直到 Spark 出现

RDD 是 Spark 的核心概念，是弹性数据集（Resilient Distributed Datasets）的缩写。RDD 既是 Spark 面向开发者的编程模型，又是 Spark 自身架构的核心元素。

Spark 上编写 WordCount 程序，主要代码只需要三行

```scala
val textFile = sc.textFile("hdfs://...")
val counts = textFile.flatMap(line => line.split(" "))
.map(word => (word, 1))
.reduceByKey(_ + _)
counts.saveAsTextFile("hdfs://...")
```

MapReduce 针对输入数据，将计算过程分为两个阶段，一个 Map 阶段，一个 Reduce 阶段，可以理解成是面向过程的大数据计算。

而 Spark 则直接针对数据进行编程，将大规模数据集合抽象成一个 RDD 对象，然后在这个 RDD 上进行各种计算处理，得到一个新的 RDD，继续计算处理，直到得到最后的结果数据。所以 Spark 可以理解成是面向对象的大数据计算。

RDD 上定义的函数分两种，一种是转换（transformation）函数，这种函数的返回值还是 RDD；另一种是执行（action）函数，这种函数不再返回 RDD。

RDD 定义了很多转换操作函数，比如有计算 map(func)、过滤 filter(func)、合并数据集 union(otherDataset)、根据 Key 聚合 reduceByKey(func, [numPartitions])、连接数据集 join(otherDataset, [numPartitions])、分组 groupByKey([numPartitions]) 等十几个函数。

## 13 | 同样的本质，为何 Spark 可以更高效？

Spark 有三个主要特性：RDD 的编程模型更简单，DAG 切分的多阶段计算过程更快速，使用内存存储中间计算结果更高效。这三个特性使得 Spark 相对 Hadoop MapReduce 可以有更快的执行速度，以及更简单的编程实现。
