---
title: Kafka核心源码解读笔记
date: 2022-07-03 14:53:05
permalink: /pages/f5f5ef/
categories:
  - 笔记
  - 分布式
  - 分布式通信
tags:
  - 分布式
  - 分布式通信
  - MQ
  - Kafka
---

# 《Kafka 核心源码解读》笔记

## 开篇词

![](https://raw.githubusercontent.com/dunwu/images/dev/snap/20220703152740.png)

从功能上讲，Kafka 源码分为四大模块。

- 服务器端源码：实现 Kafka 架构和各类优秀特性的基础。
- Java 客户端源码：定义了与 Broker 端的交互机制，以及通用的 Broker 端组件支撑代码。
- Connect 源码：用于实现 Kafka 与外部系统的高性能数据传输。
- Streams 源码：用于实现实时的流处理功能。

![](https://raw.githubusercontent.com/dunwu/images/dev/snap/20220703152803.png)

## 导读

> 构建 Kafka 工程和源码阅读环境、Scala 语言热身

kafka 项目主要目录结构

- **bin** 目录：保存 Kafka 工具行脚本，我们熟知的 kafka-server-start 和 kafka-consoleproducer 等脚本都存放在这里。
- **clients** 目录：保存 Kafka 客户端代码，比如生产者和消费者的代码都在该目录下。
- **config** 目录：保存 Kafka 的配置文件，其中比较重要的配置文件是 server.properties。
- **connect** 目录：保存 Connect 组件的源代码。我在开篇词里提到过，Kafka Connect 组件是用来实现 Kafka 与外部系统之间的实时数据传输的。
- **core** 目录：保存 Broker 端代码。Kafka 服务器端代码全部保存在该目录下。
- **streams** 目录：保存 Streams 组件的源代码。Kafka Streams 是实现 Kafka 实时流处理的组件。

## 日志段

> 保存消息文件的对象是怎么实现的？

### Kafka 日志结构

![](https://raw.githubusercontent.com/dunwu/images/dev/snap/20220704204019.png)

Kafka 日志对象由多个日志段对象组成，而每个日志段对象会在磁盘上创建一组文件，包括**消息日志文件（.log）**、**位移索引文件（.index）**、**时间戳索引文件（.timeindex）**以及已中止（Aborted）事务的**索引文件（.txnindex）**。当然，如果你没有使用 Kafka 事务，已中止事务的索引文件是不会被创建出来的。

一个 Kafka 主题有很多分区，每个分区就对应一个 Log 对象，在物理磁盘上则对应于一个子目录。比如你创建了一个双分区的主题 test-topic，那么，Kafka 在磁盘上会创建两个子目录：test-topic-0 和 test-topic-1。而在服务器端，这就是两个 **`Log`** 对象。每个子目录下存在多组日志段，也就是多组 **`.log`**、**`.index`**、**`.timeindex`** 文件组合，只不过文件名不同，因为每个日志段的起始位移不同。

### 日志段源码解析

日志段源码位于 Kafka 的 core 工程的 `LogSegment.scala` 中。该文件下定义了三个 Scala 对象：

- `LogSegment class`：日志段类
- `LogSegment object`：保存静态变量或静态方法。相当于 `LogSegment class` 的工具类。
- `LogFlushStats object`：尾部有个 stats，用于统计，负责为日志落盘进行计时。

#### LogSegment class 声明

```scala
class LogSegment private[log] (val log: FileRecords,
                               val lazyOffsetIndex: LazyIndex[OffsetIndex],
                               val lazyTimeIndex: LazyIndex[TimeIndex],
                               val txnIndex: TransactionIndex,
                               val baseOffset: Long,
                               val indexIntervalBytes: Int,
                               val rollJitterMs: Long,
                               val time: Time) extends Logging { ... }
```

参数说明：

- `log` – **包含日志条目的文件记录**。`FileRecords` 就是实际保存 Kafka 消息的对象。
- `lazyOffsetIndex` – **偏移量索引**。
- `lazyTimeIndex` – **时间戳索引**。
- `txnIndex` – **事务索引**。
- `baseOffset` – **此段中偏移量的下限**。事实上，在磁盘上看到的 Kafka 文件名就是 `baseOffset` 的值。每个 `LogSegment` 对象实例一旦被创建，它的起始位移就是固定的了，不能再被更改。
- `indexIntervalBytes` – **索引中条目之间的近似字节数**。indexIntervalBytes 值其实就是 Broker 端参数 `log.index.interval.bytes` 值，它控制了日志段对象新增索引项的频率。默认情况下，日志段至少新写入 4KB 的消息数据才会新增一条索引项。
- `rollJitterMs` – **日志段对象新增倒计时的“扰动值”**。因为目前 Broker 端日志段新增倒计时是全局设置，这就是说，在未来的某个时刻可能同时创建多个日志段对象，这将极大地增加物理磁盘 I/O 压力。有了 rollJitterMs 值的干扰，每个新增日志段在创建时会彼此岔开一小段时间，这样可以缓解物理磁盘的 I/O 负载瓶颈。
- `time` - **`Timer` 实例**。

## 参考资料

- [Kafka 核心源码解读](https://time.geekbang.org/column/intro/304)
