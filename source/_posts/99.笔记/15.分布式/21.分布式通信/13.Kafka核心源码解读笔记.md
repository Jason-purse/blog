---
title: 《Kafka 核心源码解读》笔记
date: 2022-07-03 14:53:05
permalink: /pages/f5f5ef/
categories:
  - 笔记
  - 分布式
  - 分布式通信
tags:
  - 分布式
  - 分布式通信
  - MQ
  - Kafka
---

# 《Kafka 核心源码解读》笔记

## 开篇词

![](https://raw.githubusercontent.com/dunwu/images/dev/snap/20220703152740.png)

从功能上讲，Kafka 源码分为四大模块。

- 服务器端源码：实现 Kafka 架构和各类优秀特性的基础。
- Java 客户端源码：定义了与 Broker 端的交互机制，以及通用的 Broker 端组件支撑代码。
- Connect 源码：用于实现 Kafka 与外部系统的高性能数据传输。
- Streams 源码：用于实现实时的流处理功能。

![](https://raw.githubusercontent.com/dunwu/images/dev/snap/20220703152803.png)

## 导读

> 构建 Kafka 工程和源码阅读环境、Scala 语言热身

kafka 项目主要目录结构

- **bin** 目录：保存 Kafka 工具行脚本，我们熟知的 kafka-server-start 和 kafka-consoleproducer 等脚本都存放在这里。
- **clients** 目录：保存 Kafka 客户端代码，比如生产者和消费者的代码都在该目录下。
- **config** 目录：保存 Kafka 的配置文件，其中比较重要的配置文件是 server.properties。
- **connect** 目录：保存 Connect 组件的源代码。我在开篇词里提到过，Kafka Connect 组件是用来实现 Kafka 与外部系统之间的实时数据传输的。
- **core** 目录：保存 Broker 端代码。Kafka 服务器端代码全部保存在该目录下。
- **streams** 目录：保存 Streams 组件的源代码。Kafka Streams 是实现 Kafka 实时流处理的组件。

## 日志段

> 保存消息文件的对象是怎么实现的？

### Kafka 日志结构

![](https://raw.githubusercontent.com/dunwu/images/dev/snap/20220704204019.png)

Kafka 日志对象由多个日志段对象组成，而每个日志段对象会在磁盘上创建一组文件，包括**消息日志文件（.log）**、**位移索引文件（.index）**、**时间戳索引文件（.timeindex）**以及已中止（Aborted）事务的**索引文件（.txnindex）**。当然，如果你没有使用 Kafka 事务，已中止事务的索引文件是不会被创建出来的。

一个 Kafka 主题有很多分区，每个分区就对应一个 Log 对象，在物理磁盘上则对应于一个子目录。比如你创建了一个双分区的主题 test-topic，那么，Kafka 在磁盘上会创建两个子目录：test-topic-0 和 test-topic-1。而在服务器端，这就是两个 **`Log`** 对象。每个子目录下存在多组日志段，也就是多组 **`.log`**、**`.index`**、**`.timeindex`** 文件组合，只不过文件名不同，因为每个日志段的起始位移不同。

### 日志段源码解析

日志段源码位于 Kafka 的 core 工程的 `LogSegment.scala` 中。该文件下定义了三个 Scala 对象：

- `LogSegment class`：日志段类
- `LogSegment object`：保存静态变量或静态方法。相当于 `LogSegment class` 的工具类。
- `LogFlushStats object`：尾部有个 stats，用于统计，负责为日志落盘进行计时。

#### LogSegment class 声明

```scala
class LogSegment private[log] (val log: FileRecords,
                               val lazyOffsetIndex: LazyIndex[OffsetIndex],
                               val lazyTimeIndex: LazyIndex[TimeIndex],
                               val txnIndex: TransactionIndex,
                               val baseOffset: Long,
                               val indexIntervalBytes: Int,
                               val rollJitterMs: Long,
                               val time: Time) extends Logging { ... }
```

参数说明：

- `log`：**包含日志条目的文件记录**。`FileRecords` 就是实际保存 Kafka 消息的对象。
- `lazyOffsetIndex`：**偏移量索引**。
- `lazyTimeIndex`：**时间戳索引**。
- `txnIndex`：**事务索引**。
- `baseOffset`：**此段中偏移量的下限**。事实上，在磁盘上看到的 Kafka 文件名就是 `baseOffset` 的值。每个 `LogSegment` 对象实例一旦被创建，它的起始位移就是固定的了，不能再被更改。
- `indexIntervalBytes`：**索引中条目之间的近似字节数**。indexIntervalBytes 值其实就是 Broker 端参数 `log.index.interval.bytes` 值，它控制了日志段对象新增索引项的频率。默认情况下，日志段至少新写入 4KB 的消息数据才会新增一条索引项。
- `rollJitterMs`：**日志段对象新增倒计时的“扰动值”**。因为目前 Broker 端日志段新增倒计时是全局设置，这就是说，在未来的某个时刻可能同时创建多个日志段对象，这将极大地增加物理磁盘 I/O 压力。有了 rollJitterMs 值的干扰，每个新增日志段在创建时会彼此岔开一小段时间，这样可以缓解物理磁盘的 I/O 负载瓶颈。
- `time`：**`Timer` 实例**。

#### append 方法

append 方法接收 4 个参数：分别表示

- `largestOffset`：待写入消息批次中消息的最大位移值
- `largestTimestamp`：最大时间戳
- `shallowOffsetOfMaxTimestamp`：最大时间戳对应消息的位移
- `records`：真正要写入的消息集合

![](https://raw.githubusercontent.com/dunwu/images/dev/snap/20220705062643.png)

- 第一步：在源码中，首先调用 `log.sizeInBytes` 方法判断该日志段是否为空，如果是空的话， Kafka 需要记录要写入消息集合的最大时间戳，并将其作为后面新增日志段倒计时的依据。
- 第二步：代码调用 `ensureOffsetInRange` 方法确保输入参数最大位移值是合法的。那怎么判断是不是合法呢？标准就是看它与日志段起始位移的差值是否在整数范围内，即 `largestOffset - baseOffset` 的值是不是 介于 `[0，Int.MAXVALUE]` 之间。在极个别的情况下，这个差值可能会越界，这时， `append` 方法就会抛出异常，阻止后续的消息写入。一旦你碰到这个问题，你需要做的是升级你的 Kafka 版本，因为这是由已知的 Bug 导致的。
- 第三步：待这些做完之后，`append` 方法调用 `FileRecords` 的 `append` 方法执行真正的写入。它的工作是将内存中的消息对象写入到操作系统的页缓存就可以了。
- 第四步：再下一步，就是更新日志段的最大时间戳以及最大时间戳所属消息的位移值属性。每个日志段都要保存当前最大时间戳信息和所属消息的位移信息。还记得 Broker 端提供定期删除日志的功能吗？比如我只想保留最近 7 天的日志，没错，当前最大时间戳这个值就是判断的依据；而最大时间戳对应的消息的位移值则用于时间戳索引项。虽然后面我会详细介绍，这里我还是稍微提一下：时间戳索引项保存时间戳与消息位移的对应关系。在这步操作中，Kafka 会更新并保存这组对应关系。
- 第五步：append 方法的最后一步就是更新索引项和写入的字节数了。我在前面说过，日志段每写入 4KB 数据就要写入一个索引项。当已写入字节数超过了 4KB 之后，append 方法会调用索引对象的 append 方法新增索引项，同时清空已写入字节数，以备下次重新累积计算。

#### read 方法

read 方法作用：从第一个偏移量 >= startOffset 的 Segment 开始读取消息集。如果指定了 maxOffset，则消息集将包含不超过 maxSize 字节，并将在 maxOffset 之前结束。

read 方法入参

- `startOffset`：要读取的第一条消息的位移；
- `maxSize`：能读取的最大字节数；
- `maxPosition` ：能读到的最大文件位置；
- `minOneMessage`：是否允许在消息体过大时至少返回第一条消息。

read 方法代码逻辑：

1. 调用 `translateOffset` 方法定位要读取的起始文件位置 （`startPosition`）。输入参数 `startOffset` 仅仅是位移值，Kafka 需要根据索引信息找到对应的物理文件位置才能开始读取消息。
2. 待确定了读取起始位置，日志段代码需要根据这部分信息以及 `maxSize` 和 `maxPosition` 参数共同计算要读取的总字节数。举个例子，假设 maxSize=100，maxPosition=300，startPosition=250，那么 read 方法只能读取 50 字节，因为 maxPosition - startPosition = 50。我们把它和 maxSize 参数相比较，其中的最小值就是最终能够读取的总字节数。
3. 调用 `FileRecords` 的 `slice` 方法，从指定位置读取指定大小的消息集合。

#### recover 方法

![](https://raw.githubusercontent.com/dunwu/images/dev/snap/20220705064515.png)

recover 开始时，代码依次调用索引对象的 reset 方法清空所有的索引文件，之后会开始遍历日志段中的所有消息集合或消息批次（RecordBatch）。对于读取到的每个消息集合，日志段必须要确保它们是合法的，这主要体现在两个方面：

- 该集合中的消息必须要符合 Kafka 定义的二进制格式；
- 该集合中最后一条消息的位移值不能越界，即它与日志段起始位移的差值必须是一个正整数值。

校验完消息集合之后，代码会更新遍历过程中观测到的最大时间戳以及所属消息的位移值。同样，这两个数据用于后续构建索引项。再之后就是不断累加当前已读取的消息字节数，并根据该值有条件地写入索引项。最后是更新事务型 Producer 的状态以及 Leader Epoch 缓存。不过，这两个并不是理解 Kafka 日志结构所必需的组件，因此，我们可以忽略它们。

遍历执行完成后，Kafka 会将日志段当前总字节数和刚刚累加的已读取字节数进行比较，如果发现前者比后者大，说明日志段写入了一些非法消息，需要执行截断操作，将日志段大小调整回合法的数值。同时， Kafka 还必须相应地调整索引文件的大小。把这些都做完之后，日志段恢复的操作也就宣告结束了。

## 日志

日志是日志段的容器，里面定义了很多管理日志段的操作。

![](https://raw.githubusercontent.com/dunwu/images/dev/snap/20220705195916.png)

### Log 源码结构

- `LogAppendInfo`（C）：保存消息元数据信息
- `LogAppendInfo`（O）：`LogAppendInfo`（C）工厂方法类
- `UnifiedLog`（C）：`UnifiedLog.scala` 中最核心的代码
- `UnifiedLog`（O）：`UnifiedLog`（C）工厂方法类
- `RollParams`（C）：用于控制日志段是否切分（Roll）的数据结构。
- `RollParams`（O）：RollParams 伴生类的工厂方法。
- `LogMetricNames`（O）：定义了 Log 对象的监控指标。
- `LogOffsetSnapshot`（C）：封装分区所有位移元数据的容器类。
- `LogReadInfo`（C）：封装读取日志返回的数据及其元数据。
- `CompletedTxn`（C）：记录已完成事务的元数据，主要用于构建事务索引。

### Log Class & Object

Log Object 作用：

- 定义了 Kafka 支持的文件类型
  - .log：Kafka 日志文件
  - .index：Kafka 偏移量索引文件
  - .timeindex：Kafka 时间戳索引文件
  - .txnindex：Kafka 事务索引文件
  - .snapshot：Kafka 为幂等型或事务型 Producer 所做的快照文件
  - .deleted：被标记为待删除的文件
  - .cleaned：用于日志清理的临时文件
  - .swap：将文件交换到日志中时使用的临时文件
  - -delete：被标记为待删除的目录
  - -future：用于变更主题分区文件夹地址的目录
- 定义了多种工具类方法

UnifiedLog Class 定义：

```scala
// UnifiedLog 定义
class UnifiedLog(@volatile var logStartOffset: Long,
                 private val localLog: LocalLog,
                 brokerTopicStats: BrokerTopicStats,
                 val producerIdExpirationCheckIntervalMs: Int,
                 @volatile var leaderEpochCache: Option[LeaderEpochFileCache],
                 val producerStateManager: ProducerStateManager,
                 @volatile private var _topicId: Option[Uuid],
                 val keepPartitionMetadataFile: Boolean) extends Logging with KafkaMetricsGroup { ... }

// LocalLog 定义
class LocalLog(@volatile private var _dir: File,
               @volatile private[log] var config: LogConfig,
               private[log] val segments: LogSegments,
               @volatile private[log] var recoveryPoint: Long,
               @volatile private var nextOffsetMetadata: LogOffsetMetadata,
               private[log] val scheduler: Scheduler,
               private[log] val time: Time,
               private[log] val topicPartition: TopicPartition,
               private[log] val logDirFailureChannel: LogDirFailureChannel) extends Logging with KafkaMetricsGroup { ... }
```

上面属性中最重要的两个属性是：`_dir` 和 `logStartOffset`。`_dir` 就是这个日志所在的文件夹路径，也就是主题分区的路径。`logStartOffset`，表示日志的当前最早位移。`_dir` 和 `logStartOffset` 都是 `volatile var` 类型，表示它们的值是变动的，而且可能被多个线程更新。

Log End Offset（LEO），是表示日志下一条待插入消息的位移值，而这个 Log Start Offset 是跟它相反的，它表示日志当前对外可见的最早一条消息的位移值。

![](https://raw.githubusercontent.com/dunwu/images/dev/snap/20220705201758.png)

图中绿色的位移值 3 是日志的 Log Start Offset，而位移值 15 表示 LEO。另外，位移值 8 是高水位值，它是区分已提交消息和未提交消息的分水岭。

有意思的是，Log End Offset 可以简称为 LEO，但 Log Start Offset 却不能简称为 LSO。因为在 Kafka 中，LSO 特指 Log Stable Offset，属于 Kafka 事务的概念。

Log 类的其他属性你暂时不用理会，因为它们要么是很明显的工具类属性，比如 timer 和 scheduler，要么是高阶用法才会用到的高级属性，比如 producerStateManager 和 logDirFailureChannel。工具类的代码大多是做辅助用的，跳过它们也不妨碍我们理解 Kafka 的核心功能；而高阶功能代码设计复杂，学习成本高，性价比不高。

其他一些重要属性：

- nextOffsetMetadata：它封装了下一条待插入消息的位移值，你基本上可以把这个属性和 LEO 等同起来。
- highWatermarkMetadata：是分区日志高水位值。
- segments：我认为这是 Log 类中最重要的属性。它保存了分区日志下所有的日志段信息，只不过是用 Map 的数据结构来保存的。Map 的 Key 值是日志段的起始位移值，Value 则是日志段对象本身。Kafka 源码使用 ConcurrentNavigableMap 数据结构来保存日志段对象，就可以很轻松地利用该类提供的线程安全和各种支持排序的方法，来管理所有日志段对象。
- Leader Epoch Cache 对象。Leader Epoch 是社区于 0.11.0.0 版本引入源码中的，主要是用来判断出现 Failure 时是否执行日志截断操作（Truncation）。之前靠高水位来判断的机制，可能会造成副本间数据不一致的情形。这里的 Leader Epoch Cache是一个缓存类数据，里面保存了分区 Leader 的 Epoch 值与对应位移值的映射关系，我建议你查看下 LeaderEpochFileCache 类，深入地了解下它的实现原理。

### lOG 类初始化逻辑

![](https://raw.githubusercontent.com/dunwu/images/dev/snap/20220705204919.png)

## 参考资料

- [Kafka 核心源码解读](https://time.geekbang.org/column/intro/304)
