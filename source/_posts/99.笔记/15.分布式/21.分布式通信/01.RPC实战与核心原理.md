---
title: RPC实战与核心原理
date: 2022-06-19 09:48:17
categories:
  - 笔记
  - 分布式
  - 分布式通信
tags:
  - 分布式
  - 分布式通信
  - RPC
permalink: /pages/4b43b4/
---

# RPC 实战与核心原理

为什么要学习 RPC

RPC 不仅是微服务的架构基础，实际上，只要涉及网络通信，就可能用到 RPC。

例 1：大型分布式应用系统可能会依赖消息队列、分布式缓存、分布式数据库以及统一配置
中心等，应用程序与依赖的这些中间件之间都可以通过 RPC 进行通信。比如 etcd，它作为
一个统一的配置服务，客户端就是通过 gRPC 框架与服务端进行通信的。

例 2：我们经常会谈到的容器编排引擎 Kubernetes，它本身就是分布式的，Kubernetes
的 kube-apiserver 与整个分布式集群中的每个组件间的通讯，都是通过 gRPC 框架进行
的。

RPC 是解决分布式系统通信问题的一大利器。

## 核心原理

### 什么是 RPC？

RPC 的全称是 Remote Procedure Call，即远程过程调用。

RPC 的作用体现在两个方面：

- 屏蔽远程调用跟本地调用的差异，让用户像调用本地一样去调用远程方法。
- 隐藏底层网络通信的复杂性，让用户更专注于业务逻辑。

### RPC 通信流程

![](https://raw.githubusercontent.com/dunwu/images/dev/snap/20220619100051.png)

RPC 是一个远程调用，因此必然需要通过网络传输数据，且 RPC 常用于业务系统之间的数据交互，需要保证其可靠性，所以 RPC 一般默认采用 **TCP** 来传输。

网络传输数据是二进制数据，因此请求方需要将请求参数转为二进制数据，即**序列化**。

响应方接受到请求，要将二进制数据转换为请求参数，需要**反序列化**。

请求方和响应方识别彼此的信息，需要约定好彼此数据的格式，即**协议**。大多数的协议会分成两部分，分别是数据头和消息体。数据头一般用于身份识别，包括协议标识、数据大小、请求类型、序列化类型等信息；消息体主要是请求的业务参数信息和扩展属性等。

为了屏蔽底层通信细节，使用户聚焦自身业务，因此 RPC 框架一般引入了动态代理，通过依赖注入等技术，拦截方法调用，完成远程调用的通信逻辑。

### RPC 在架构中的位置

RPC 框架能够帮助我们解决系统拆分后的通信问题，并且能让我们像调用本地一样去调用
远程方法。

![](https://raw.githubusercontent.com/dunwu/images/dev/snap/20220619101023.png)

## 协议

### 协议的作用

在传输过程中，RPC 并不会把请求参数的所有二进制数据整体一下子发送到对端机器上，中间可能会拆分成好几个数据包，也可能会合并其他请求的数据包（合并的前提是同一个 TCP 连接上的数据），至于怎么拆分合并，这其中的细节会涉及到系统参数配置和 TCP 窗口大小。对于服务提供方应用来说，他会从 TCP 通道里面收到很多的二进制数据，那这时候怎么识别出哪些二进制是第一个请求的呢？

这就好比让你读一篇没有标点符号的文章，你要怎么识别出每一句话到哪里结束呢？很简单啊，我们加上标点，完成断句就好了。

为了避免语义不一致的事情发生，我们就需要在发送请求的时候设定一个边界，然后在收到请求的时候按照这个设定的边界进行数据分割。这个边界语义的表达，就是我们所说的协议。

### 为何需要设计 RPC 协议

有了现成的 HTTP 协议，为啥不直接用，还要为 RPC 设计私有协议呢？

RPC 更多的是负责应用间的通信，所以性能要求相对更高。但 HTTP 协议的数据包大小相对请求数据本身要大很多，又需要加入很多无用的内容，比如换行符号、回车符等；还有一个更重要的原因是，HTTP 协议属于无状态协议，客户端无法对请求和响应进行关联，每次请求都需要重新建立连接，响应完成后再关闭连接。因此，对于要求高性能的 RPC 来说，HTTP 协议基本很难满足需求，所以 RPC 会选择设计更紧凑的私有协议。

### 如何设计 RPC 协议？

首先，必须先明确消息的边界，即确定消息的长度。因此，至少要分为：消息长度+消息内容两部分。

接下来，我们会发现，在使用过程中，仅消息长度，不足以明确通信中的很多细节：如序列化方式是怎样的？是否消息压缩？压缩格式是怎样的？如果协议发生变化，需要明确协议版本等等。

综上，一个 RPC 协议大概会由下图中的这些参数组成：

![](https://raw.githubusercontent.com/dunwu/images/dev/snap/20220619102052.png)

### 可扩展的协议

前面所述的协议属于定长协议头，那也就是说往后就不能再往协议头里加新参数了，如果加参
数就会导致线上兼容问题。

为了保证能平滑地升级改造前后的协议，我们有必要设计一种支持可扩展的协议。其关键在于让协议头支持可扩展，扩展后协议头的长度就不能定长了。那要实现读取不定长的协议头里面的内容，在这之前肯定需要一个固定的地方读取长度，所以我们需要一个固定的写入协议头的长度。整体协议就变成了三部分内容：固定部分、协议头内容、协议体内容。

![](https://raw.githubusercontent.com/dunwu/images/dev/snap/20220619102833.png)

## 序列化

在不同的场景下合理地选择序列化方式，对提升 RPC 框架整体的稳定性和性能是至关重要的。

![](https://raw.githubusercontent.com/dunwu/images/dev/snap/20220619101617.png)

序列化就是将对象转换成二进制数据的过程，而反序列就是反过来将二进制转换为对象的过程。

![](https://raw.githubusercontent.com/dunwu/images/dev/snap/20220619104420.png)

常用序列化方式

- JDK 序列化：`ObjectInputStream` 和 `ObjectOutputStream`
- JSON
- 二进制
  - Hessian
  - Protobuf
  - Thirft

**RPC 协议选型**

优先级依次从高到低：安全性、通用性、兼容性、性能、效率、空间开销。

这归根结底还是因为服务调用的稳定性与可靠性，要比服务的性能与响应耗时更加重要。另
外对于 RPC 调用来说，整体调用上，最为耗时、最消耗性能的操作大多都是服务提供者执
行业务逻辑的操作，这时序列化的开销对于服务整体的开销来说影响相对较小。

**使用 RPC 需要注意哪些问题**

1. 对象要尽量简单，没有太多的依赖关系，属性不要太多，尽量高内聚；
2. 入参对象与返回值对象体积不要太大，更不要传太大的集合；
3. 尽量使用简单的、常用的、开发语言原生的对象，尤其是集合类；
4. 对象不要有复杂的继承关系，最好不要有父子类的情况。

## 网络通信

常见的网络 IO 模型分为四种：同步阻塞 IO（BIO）、同步非阻塞 IO（NIO）、IO 多路复
用和异步非阻塞 IO（AIO）。在这四种 IO 模型中，只有 AIO 为异步 IO，其他都是同步
IO。

IO 多路复用（Reactor 模式）在高并发场景下使用最为广泛，很多知名软件都应用了这一技术，如：Netty、Redis、Nginx 等。

IO 多路复用分为 select，poll 和 epoll。

什么是 IO 多路复用？字面上的理解，多路就是指多个通道，也就是多个网络连接的 IO，而复用就是指多个通道复用在一个复用器上。

### 零拷贝

系统内核处理 IO 操作分为两个阶段——等待数据和拷贝数据。等待数据，就是系统内核在等待网卡接收到数据后，把数据写到内核中；而拷贝数据，就是系统内核在获取到数据后，将数据拷贝到用户进程的空间中。

网络 IO 读写流程

![](https://raw.githubusercontent.com/dunwu/images/dev/snap/20220619174154.png)

应用进程的每一次写操作，都会把数据写到用户空间的缓冲区中，再由 CPU 将数据拷贝到系统内核的缓冲区中，之后再由 DMA 将这份数据拷贝到网卡中，最后由网卡发送出去。这里我们可以看到，一次写操作数据要拷贝两次才能通过网卡发送出去，而用户进程的读操作则是将整个流程反过来，数据同样会拷贝两次才能让应用程序读取到数据。

应用进程的一次完整的读写操作，都需要在用户空间与内核空间中来回拷贝，并且每一次拷贝，都需要 CPU 进行一次上下文切换（由用户进程切换到系统内核，或由系统内核切换到用户进程），这样很浪费 CPU 和性能。

所谓的零拷贝，就是取消用户空间与内核空间之间的数据拷贝操作，应用进程每一次的读写操作，可以通过一种方式，直接将数据写入内核或从内核中读取数据，再通过 DMA 将内核中的数据拷贝到网卡，或将网卡中的数据 copy 到内核。

![](https://raw.githubusercontent.com/dunwu/images/dev/snap/20220619174335.png)

Netty 的零拷贝偏向于用户空间中对数据操作的优化，这对处理 TCP 传输中的拆包粘包问题有着重要的意义，对应用程序处理请求数据与返回数据也有重要的意义。

Netty 框架中很多内部的 ChannelHandler 实现类，都是通过 CompositeByteBuf、slice、wrap 操作来处理 TCP 传输中的拆包与粘包问题的。

Netty 的 ByteBuffer 可以采用 Direct Buffers，使用堆外直接内存进行 Socketd 的读写
操作，最终的效果与我刚才讲解的虚拟内存所实现的效果是一样的。

Netty 还提供 FileRegion 中包装 NIO 的 FileChannel.transferTo() 方法实现了零拷
贝，这与 Linux 中的 sendfile 方式在原理上也是一样的。

## 动态代理

动态代理可以帮用户屏蔽远程调用的细节，实现像调用本地一样地调用远程的体验。

![](https://raw.githubusercontent.com/dunwu/images/dev/snap/20220619204255.png)

JDK 支持的动态代理方式是通过实现 InvocationHandler 接口。这种方式有一定的局限性——它要求被代理的类只能是接口。原因是因为生成的代理类会继承 Proxy 类，但 Java 是不支持多重继承的。此外，它还有性能问题。它生成后的代理类是使用反射来完成方法调用的，而这种方式相对直接用编码调用来说，性能会降低。

除 JDK 以外，还有其他第三方框架可以实现动态代理，如像 Javassist、Byte Buddy。

Javassist 的是通过控制底层字节码来实现动态代理，不需要反射完成调用，所以性能肯定比 JDK 的动态代理方式性能要好。

Byte Buddy 则属于后起之秀，在很多优秀的项目中，像 Spring、Jackson 都用到了 Byte Buddy 来完成底层代理。相比 Javassist，Byte Buddy 提供了更容易操作的 API，编写的代码可读性更高。更重要的是，生成的代理类执行速度比 Javassist 更快。

## RPC 实战

略

## 架构设计

### RPC 架构

**其实 RPC 就是把拦截到的方法参数，转成可以在网络中传输的二进制，并保证在服务提供方能正确地还原出语义，最终实现像调用本地一样地调用远程的目的**。

RPC 本质上就是一个远程调用，必然需要通过网络来传输数据，为了屏蔽网络传输的复杂性，需要封装一个单独的**数据传输模块**用来收发二进制数据。

用户请求的时候是基于方法调用，方法出入参数都是对象数据，对象是肯定没法直接在网络中传输的，我们需要提前把它转成可传输的二进制，这就是我们说的序列化过程。但只是把方法调用参数的二进制数据传输到服务提供方是不够的，我们需要在方法调用参数的二进制数据后面增加“断句”符号来分隔出不同的请求，在两个“断句”符号中间放的内容就是我们请求的二进制数据，这个过程我们叫做协议封装。可以把这两个处理过程放在同一个模块，统称为**协议模块**。除此之外，我们还可以在协议模块中加入压缩功能，这是因为压缩过程也是对传输的二进制数据进行操作。

RPC 还需要为调用方找到所有的服务提供方，并需要在 RPC 里面维护好接口跟服务提供者地址的关系，这样调用方在发起请求的时候才能快速地找到对应的接收地址，这个过程即为“服务发现”。

但服务发现只是解决了接口和服务提供方地址映射关系的查找问题。但是，对于 RPC 来说，每次发送请求的时候都是需要用 TCP 连接的，相对服务提供方 IP 地址，TCP 连接状态是瞬息万变的，所以我们的 RPC 框架里面要有连接管理器去维护 TCP 连接的状态。

有了集群之后，提供方可能就需要管理好这些服务了，那我们的 RPC 就需要内置一些服务治理的功能，比如服务提供方权重的设置、调用授权等一些常规治理手段。而服务调用方需要额外做哪些事情呢？每次调用前，我们都需要根据服务提供方设置的规则，从集群中选择可用的连接用于发送请求。

![](https://raw.githubusercontent.com/dunwu/images/dev/snap/20220620112739.png)

### RPC 可扩展架构

在 RPC 框架中，如何支持插件化架构呢？

可以使用 SPI 技术来实现。注意：由于 JDK SPI 性能不高，并且不支持自动注入，所以，一般会选择其他的 SPI 实现。

![](https://raw.githubusercontent.com/dunwu/images/dev/snap/20220620113147.png)

有了 SPI 支持插件式加载后，RPC 框架就变成了一个微内核架构。

## 服务发现

![](https://raw.githubusercontent.com/dunwu/images/dev/snap/20220620144009.png)

RPC 框架必须要有服务注册和发现机制，这样，集群中的节点才能知道通信方的请求地址。

- **服务注册**：在服务提供方启动的时候，将对外暴露的接口注册到注册中心之中，注册中心将这个服务节点的 IP 和接口保存下来。
- **服务订阅**：在服务调用方启动的时候，去注册中心查找并订阅服务提供方的 IP，然后缓存到本地，并用于后续的远程调用。

### 基于 ZooKeeper 的服务发现

使用 ZooKeeper 作为服务注册中心，是 Java 分布式系统的经典方案。

搭建一个 ZooKeeper 集群作为注册中心集群，服务注册的时候只需要服务节点向 ZooKeeper 节点写入注册信息即可，利用 ZooKeeper 的 Watcher 机制完成服务订阅与服务下发功能

![img](https://raw.githubusercontent.com/dunwu/images/dev/snap/20200610180056.png)

通常我们可以使用 ZooKeeper、etcd 或者分布式缓存（如 Hazelcast）来解决事件通知问题，但当集群达到一定规模之后，依赖的 ZooKeeper 集群、etcd 集群可能就不稳定了，无法满足我们的需求。

在超大规模的服务集群下，注册中心所面临的挑战就是超大批量服务节点同时上下线，注册中心集群接受到大量服务变更请求，集群间各节点间需要同步大量服务节点数据，最终导致如下问题：

- 注册中心负载过高；
- 各节点数据不一致；
- 服务下发不及时或下发错误的服务节点列表。

RPC 框架依赖的注册中心的服务数据的一致性其实并不需要满足 CP，只要满足 AP 即可。

### 基于消息总线的最终一致性的注册中心

ZooKeeper 的一大特点就是强一致性，ZooKeeper 集群的每个节点的数据每次发生更新操作，都会通知其它 ZooKeeper 节点同时执行更新。它要求保证每个节点的数据能够实时的完全一致，这也就直接导致了 ZooKeeper 集群性能上的下降。

而 RPC 框架的服务发现，在服务节点刚上线时，服务调用方是可以容忍在一段时间之后
（比如几秒钟之后）发现这个新上线的节点的。毕竟服务节点刚上线之后的几秒内，甚至更长的一段时间内没有接收到请求流量，对整个服务集群是没有什么影响的，所以我们可以牺牲掉 CP（强制一致性），而选择 AP（最终一致），来换取整个注册中心集群的性能和稳定性。

![img](https://raw.githubusercontent.com/dunwu/images/dev/snap/20200717162006.png)

## 健康检测

## 路由策略

## 负载均衡

## 异常重试

## 优雅关闭

## 优雅启动

## 熔断限流

## 业务分组

## 异步 RPC

## 安全体系

## 问题定位

## 时钟轮

## 流量回放

## 动态分组

## 参考资料

- [《RPC 实战与核心原理》](https://time.geekbang.org/column/intro/280)
